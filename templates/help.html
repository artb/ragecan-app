{% extends 'base.html' %}

{% block content %}
<div class="container mt-4">
    <h1 class="text-left">Help and Glossary</h1>
    <div class="row justify-content-center">
        <div class="col-md-12">
            <div class="p-5 bg-white shadow rounded">
                <p class="placeholder-glow">
                    <span>The RAGECAN Framework is a tool that combines the easy use of dimensionality reduction methods with a simplified graphical interface, aiming to provide a simplified and straighforward way of using dimensionality reduction techniques to mitigate the problem of the curse of dimensionality in gene expression databases. RAGECAN is a framework that implements validation techniques, parameter optimization and the search for the dimensionality that maximizes classification results. In order to provide the scientific community with an approach that abstracts the complexity of developing the entire process mentioned above, we created a mechanism based on binary search to find the ideal dimension. This pipeline has been named BSFS (Binary Search Feature Selection) and is integrated into the main page of this GUI.</span>
                    <span>To run the pipeline, simply insert the gene expression data in .csv format, indicate the column with the labels, select the classifier and metric used to extract the results and after selecting the dimensional reduction methods that will be used, and finally,  just click start! In the experiments tab, the framework is configured to present an interface with MlFlow for tracking the results and analyzing the metrics of each run made during the search process. Finally, the tool generates a report on the dimensional reduction process and exports the dataset in reduced dimensions. To facilitate the choice of methods, we have summarized the ten dimensional reduction techniques developed for this framework below.</span>
                    <ul>
                        <li><b>ANOVA</b> (Analysis of Variance) is a statistical method used to identify features that significantly impact the target variable. It compares the variances between groups to the variance within groups to see if the mean differences are statistically significant. This method is light and effective in finding features that are likely to influence the target in linear models.</li>
                        <li><b>Information Gain</b> (IG) is a feature selection method that measures the reduction in uncertainty about the target variable when a feature is known. It compares the entropy of the target before and after observing the feature, selecting features that provide the most informative splits for classification or regression tasks.</li>
                        <li><b>Chi-square</b> feature selection method evaluates the relationship between categorical features and the target variable. It measures the association by comparing the observed and expected frequencies of each feature-target combination. Features with a significant difference are chosen, indicating they strongly influence on the target class.</li>
                        <li><b>mRMR</b> (Minimum Redundancy Maximum Relevance) method selects features that are highly relevant to the target while minimizing redundancy between them. It ranks features based on their relevance to the target and selects those that contribute new, non-redundant information to the predictive model. This method strikes a balance between maximizing useful information and reducing repetitive features.</li>
                        <li><b>PCA</b> (Principal Component Analysis) is a feature extracion method that reduces dimensionality by transforming features into fewer, uncorrelated principal components. These components capture the maximum variance in the data, helping to preserve important information while reducing the feature set, ideal for visualizing and simplifying complex datasets.</li>
                        <li><b>Autoencoder</b> is a neural network used for feature selection by compressing data into a smaller representation (encoding) and then reconstructing the original input (decoding). The encoded layer represents the essential features, filtering out noise and irrelevant information to simplify data while retaining its most important aspects. As this is a deep method, we recommend that it is only used by users with a GPU configured for this type of calculation.</li>
                        <li><b>Denoising Autoencoder</b> improves upon traditional autoencoders by learning to recover the original input from corrupted versions. It's trained to remove noise, helping to capture the true underlying structure of the data. This method enhances the robustness and quality of the features extracted, making them more useful for predictive modeling. As this is a deep method, we recommend that it is only used by users with a GPU configured for this type of calculation.</li>
                        <li><b>Siamese Networks</b> with a semihard triplet loss function compress features like an autoencoder but sort them using the triplet loss function. They organize embeddings by comparing anchor, positive, and negative pairs, ensuring similar features are grouped together while dissimilar ones are pushed apart. This approach creates creates clusters of data with similar characteristics based on their class, allowing for the creation of more meaningful representations for the classifier. Since this is a deep method, we recommend that it is only used by users with a GPU configured for this type of calculation.</li>
                        <li><b>RFE</b> (Recursive Feature Elimination) iteratively removes the least important features to reduce the feature set. It ranks features by training a classifier, in our case RandomForest and SVM, removes the weakest features, and repeats this process until the desired number is reached, ensuring the remaining features have the highest predictive power.</li>
                    </ul>
                </p>
            </div>
        </div>
    </div>
</div>
{% endblock %}
